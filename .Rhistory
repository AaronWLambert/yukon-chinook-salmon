# # PSS days included up to myDay
# #  I.e., June 1 = 1, June 2 = 2 ....
# dayPSS <- unique(PSS_hist$Day[PSS_hist$Day <=(myDay)])-147 *******
# dayPSS <- unique(PSS_hist$Day[PSS_hist$Day <=(myDay) & PSS_hist$Day>= startDayPSS])-147
dayPSS <- unique(PSS_hist$Day[PSS_hist$Day <=(myDay) &
PSS_hist$Day>= startDayPSS])
#
dayPSS_all <- c(startDayPSS:endDayPSS)
loc.allDays.myDay <- which(dayPSS_all %in% myDay)
# Number of days used
n_dayPSS <- length(dayPSS)
n_dayPSS_all <- length(startDayPSS:endDayPSS )
curr_PSS <- (PSS_hist$count[PSS_hist$Year == myYear &
PSS_hist$Day <= myDay &
PSS_hist$Day>=startDayPSS])
names(curr_PSS) <- (PSS_hist$Day[PSS_hist$Year == myYear &
PSS_hist$Day <= myDay &
PSS_hist$Day>=startDayPSS])
# Number of days used
n_curr_PSS <- length(curr_PSS)
# # TAKE ME OUT##
cum_curr_PSS <- cumsum(curr_PSS)
# Cumulative current PSS counts
cum_curr_PSS <- cumsum(curr_PSS)
# Create matrix with dimensions days x historic Year
PSS_mat <- matrix(nrow = n_dayPSS,
#start 1996 to account for missing year 1996
ncol = n_yearPSS)
# Give names to matrix for accounting
colnames(PSS_mat) <- yearPSS
rownames(PSS_mat) <- dayPSS
# Create vector of counts from relevant years and days to input into matrix
# Include years up to the latest year with EOS reconstructed count (max(CAN_hist$year))
(count_vect <- PSS_hist$count[PSS_hist$Year != myYear &
PSS_hist$Year >= startYearPSS &
PSS_hist$Year <= max(CAN_hist$Year) &
PSS_hist$Day <= (myDay) &
PSS_hist$Day >= (startDayPSS)])
# Use loop to populate matrix with counts by days
# Set counter
counter <-1
for (y in 1:n_yearPSS){
for (d in 1:n_dayPSS) {
PSS_mat[d,y] <- count_vect[counter]
counter = counter+1
}
}
# Cumulative PSS matrix
cum_PSS_mat <-  apply(PSS_mat, 2, cumsum)
# Create matrix of PSS for every day in the season
PSS_mat_all <- matrix(nrow = n_dayPSS_all,
#start 1996 to account for missing year 1996
ncol = n_yearPSS)
# Give names to matrix for accounting
colnames(PSS_mat_all) <- yearPSS
rownames(PSS_mat_all) <- c(startDayPSS:endDayPSS)
# Create vector of counts from relevant years and days to input into matrix
(count_vect_all <- PSS_hist$count[PSS_hist$Year != myYear &
PSS_hist$Year >= startYearPSS &
PSS_hist$Day <= endDayPSS &
PSS_hist$Day >= startDayPSS
])
# The number of observations in count_vector for PSS historic counts
(n_hist_counts_all <- length(count_vect_all))
# Use loop to populate matrix with counts by days
# Set counter
counter <-1
for (y in 1:n_yearPSS){
for (d in 1:n_dayPSS_all) {
PSS_mat_all[d,y] <- count_vect_all[counter]
counter = counter+1
}
}
# Matrix of cumulative pss pasage by day and year
cum_PSS_mat_all <- apply(PSS_mat_all,MARGIN = 2, FUN = cumsum)
# Copy matrix
PSS_mat_prop_all <- PSS_mat_all
# Populate with NA's
PSS_mat_prop_all[,] <- NA
# Fill with observed proportions by day and year
for (y in 1:n_yearPSS){
for (d in 1:n_dayPSS_all) {
PSS_mat_prop_all[d,y] <- cum_PSS_mat_all[d,y]/ cum_PSS_mat_all[n_dayPSS_all,y]
}
}
# Sum each years except myYear PSS passage up to myDay
cumPSS <- apply(PSS_mat, 2, sum)
obs.prop.PSS <- cumPSS/totalEOS
mean_propPSS <- mean(obs.prop.PSS)
# Vector of historic Eagle Sonar passage excluding myYear
yearEagle <- unique(Eagle_hist$Year[Eagle_hist$Year != myYear])
# Number of Eagle sonar years used in model
n_yearEagle <- length(yearEagle)
dayEagle <- unique(Eagle_hist$Day[Eagle_hist$Day <=(myDay) &
Eagle_hist$Day>= startDayPSS])
# Number of days used
n_dayEagle <- length(dayEagle)
# Eagle daily passage estimate for days up to myDay for the year of interest (myYear)
# curr_PSS <- (PSS_hist$count[PSS_hist$Year == myYear & PSS_hist$Day <= myDay ]) **********
curr_Eagle_vect <- (Eagle_hist$count[Eagle_hist$Year == myYear &
Eagle_hist$Day <= myDay &
Eagle_hist$Day>=startDayPSS])
curr_Eagle <- array(data = curr_Eagle_vect,
dim = length(curr_Eagle_vect))
# Number of days used
n_curr_Eagle <- length(curr_Eagle)
# Cumulative Eagle counts for myYear
cum_curr_Eagle <- cumsum(curr_Eagle)
#
loc.eagle.years <- which(yearPSS %in% yearEagle)
loc.eagle.years
loc.PF.years.eagle <- which(yearEagle %in% yearPF)
loc.PF.years.PSS <- which(yearPSS %in% yearPF)
# Create matrix with dimensions myday and myYear
Eagle_mat <- matrix(nrow = n_dayPSS,
#start 1996 to account for missing year 1996
ncol = n_yearEagle)
# Give names to matrix for accounting
colnames(Eagle_mat) <- yearEagle
rownames(Eagle_mat) <- dayPSS
# Create vector of counts from relevant years and days to input into matrix
# (count_vect <- PSS_hist$count[PSS_hist$Year<= (myYear-1) & *******************************************
#                                 PSS_hist$Day <= (myDay)])
(count_vect_Eagle <- Eagle_hist$count[Eagle_hist$Year != myYear &
Eagle_hist$Day <= (myDay) &
Eagle_hist$Day >= (startDayPSS)])
# The number of observations in count_vector for PSS historic counts
n_hist_Eaglecounts <- length(count_vect_Eagle)
# This is to index where to start filling the Eagle matrix
SE <- n_dayPSS- n_dayEagle + 1
#Counter for loop
counter <-1
# Loop to fill matix by year and day
for (y in 1:n_yearEagle){
for (d in SE:n_dayPSS) {
Eagle_mat[d,y] <- count_vect_Eagle[counter]
counter = counter+1
}
}
rstan:::rstudio_stanc("stan/Yukon Inseason Forecast 6.prop.stan")
# Fill in the rest of the matrix with zeros
Eagle_mat[is.na(Eagle_mat)]<-0
# Cumulative count matrix for historic years
Eagle_cum_hist_mat <- apply(X = Eagle_mat,MARGIN = 2,FUN = cumsum)
# Cumulative Counts by year for plotting
cumEagle <- apply(Eagle_mat,MARGIN = 2,sum)
names(cumEagle) <- yearEagle
# Calculate the proportion for day D for each year Y
prop_Eagle <- cumEagle/totalEOS[loc.eagle.years]
mean_prop_Eagle <- mean(prop_Eagle)
# Vector containing avg GSI proportions for each day ACROSS ALL YEARS (for versions 3.0,3.1,3.2,3.3) ####
meanGSI_vect <- vector(length = length(startDayPSS:myDay))
names(meanGSI_vect) <- c(startDayPSS:myDay)
# Vector of GSI sd
sdGSI_vect <- vector(length = length(startDayPSS:myDay))
names(sdGSI_vect) <- c(startDayPSS:myDay)
counter <- 1
for (d in startDayPSS:myDay) {
# d = 175
meanGSI_vect[counter]<- mean(GSI_by_year$propCan[GSI_by_year$startday <= d &
GSI_by_year$endday >=d &
GSI_by_year$year != myYear &
GSI_by_year$year != 2013])
sdGSI_vect[counter]<- sd(GSI_by_year$propCan[GSI_by_year$startday <= d &
GSI_by_year$endday >=d &
GSI_by_year$year != myYear&
GSI_by_year$year != 2013])
counter <- counter+1
}
# No GSI data for day 148 and 149. Use day 150 for these values
meanGSI_vect[1:2] <- meanGSI_vect[3]
sdGSI_vect[1:2] <- sdGSI_vect[3]
# Mean GSI by mean strata dates. This is used in versions 3.4 and up #######################
meanStartDay <- GSI_by_year %>%
filter(year!= myYear & year != 2013) %>%
group_by(stratum) %>%
summarise("meanStartDay" = mean(startday)) %>%
as.data.frame()
GSI_mean_by_strata <- GSI_by_year %>%
filter(year!= myYear & year != 2013) %>%
summarize("stratumMean" = c(mean(propCan[stratum == 1]),
mean(propCan[stratum == 2]),
mean(propCan[stratum == 3 | stratum == 4])),
"stratumSD" = c(sd(propCan[stratum == 1]),
sd(propCan[stratum == 2]),
sd(propCan[stratum == 3 | stratum == 4]))) %>%  as.data.frame()
GSI <- cbind(GSI_mean_by_strata,round(meanStartDay[1:3,]))
GSI_avg <-c(rep(GSI$stratumMean[GSI$stratum == 1],
times = length(startDayPSS:GSI$meanStartDay[GSI$stratum==2]-1)),
rep(GSI$stratumMean[GSI$stratum == 2],
times = length(GSI$meanStartDay[GSI$stratum==2]:GSI$meanStartDay[GSI$stratum == 3]-1)),
rep(GSI$stratumMean[GSI$stratum == 3],
times = length(GSI$meanStartDay[GSI$stratum == 3]:max(PSS_hist$Day))))
GSI_avg_vect <- GSI_avg[1:n_dayPSS]
GSI_sd <- c(rep(GSI$stratumSD[GSI$stratum == 1],
times = length(startDayPSS:GSI$meanStartDay[GSI$stratum==2]-1)),
rep(GSI$stratumSD[GSI$stratum == 2],
times = length(GSI$meanStartDay[GSI$stratum==2]:GSI$meanStartDay[GSI$stratum == 3]-1)),
rep(GSI$stratumSD[GSI$stratum == 3],
times = length(GSI$meanStartDay[GSI$stratum == 3]:max(PSS_hist$Day))))
GSI_sd_vect <- GSI_sd[1:n_dayPSS]
# Create matrix with dimensions myday and myYear
PSS_mat_adj <- matrix(nrow = n_dayPSS,
ncol = n_yearPSS)
# Give names to matrix
colnames(PSS_mat_adj) <- yearPSS
rownames(PSS_mat_adj) <- dayPSS
for (y in 1:n_yearPSS) {
for (d in 1:n_dayPSS) {
# y = 1
# d = 15
PSS_mat_adj[d,y] <- PSS_mat[d,y]*meanGSI_vect[d]
}
}
# Cumulative GSI adjusted historic counts
cumPSS_adj <- apply(PSS_mat_adj, 2, sum)
adj_curr_PSS <- vector(length = n_dayPSS)
for (d in 1:n_dayPSS) {
adj_curr_PSS[d] <- curr_PSS[d]*meanGSI_vect[d]
}
# GSI Beta parameters
meanpropCAN <- meanGSI_vect
sd_meanpropCAN <- sdGSI_vect
paramA <- vector(length = n_dayPSS)
paramB <- vector(length = n_dayPSS)
for(x in 1:n_dayPSS){
# x = 1
paramA[x] <- ((1-meanpropCAN[x])/(sd_meanpropCAN[x]^2)-(1/meanpropCAN[x]))*meanpropCAN[x]^2;
paramB[x] <- paramA[x] * ((1/meanpropCAN[x])-1);
}
# PSS var calculations for model incorporating uncertainty #################################
tempSD <- PSS_sd[PSS_sd$Day<= myDay &
PSS_sd$Year != myYear,] %>%
group_by(Year) %>%
summarise(sd = sqrt(sum(Var)))
full_tempSD <- data.frame("Year" = yearPSS, "sd" = 0)
full_tempSD$sd <- tempSD$sd[match(full_tempSD$Year,tempSD$Year)]
full_tempSD$sd[ is.na(full_tempSD$sd)] <- 0
PSS_year_sd <- full_tempSD$sd
names(PSS_year_sd) <- yearPSS
curr_PSS_year_sd <- sqrt(sum(PSS_sd$Var[PSS_sd$Day<= myDay &
PSS_sd$Year == myYear] ))
may.sst.hist <- norton.sst$month.mean[norton.sst$month == 5 &
norton.sst$year != 1996 &
# norton.sst$year != 2020 &
# norton.sst$year >= startYearPSS &
norton.sst$year != myYear]
yearSST <- norton.sst$year[norton.sst$month == 5 &
norton.sst$year != 1996 &
# norton.sst$year != 2020 &
# norton.sst$year >= startYearPSS &
norton.sst$year != myYear]
n_yearSST <- length(yearSST)
may.sst.curr <-norton.sst$month.mean[norton.sst$month == 5 &
norton.sst$year == myYear]
yearSST <- norton.sst$year[norton.sst$month == 5 &
norton.sst$year != 1996 &
# norton.sst$year != 2020 &
# norton.sst$year >= startYearPSS &
norton.sst$year != myYear]
n_sst <- length(may.sst.hist)
# Get vector of historic midpoints for all years excluding myYear
hist.midpoint <- logistic.all$mid[logistic.all$year != myYear
# & logistic.all$year >= startYearPSS
]
# Name eache entry with the year
names(hist.midpoint) <-logistic.all$year[logistic.all$year != myYear
# & logistic.all$year >= startYearPSS
]
# Get historic mean mp
avg_midpoint <- mean(hist.midpoint)
log_mid <- logistic.mean["m"]
log_s <- logistic.mean["s"]
# Calculate historic deviations from the mean mp
deviations.mp <- hist.midpoint - avg_midpoint
loc.devMP.allYears <- which(yearSST %in% yearPSS)
# Store each parameter as a vector for model input
(ps_mu <- normal.all$mid[normal.all$year != myYear &
normal.all$year >= startYearPSS])
(ps_sd <- normal.all$sd[normal.all$year != myYear &
normal.all$year >= startYearPSS])
(ps_alpha_norm <- normal.all$alpha[normal.all$year != myYear&
normal.all$year >= startYearPSS])
n_ps_mu <- length(ps_mu)
n_ps_sd <- length(ps_sd)
n_ps_alpha <- length(ps_alpha_norm)
ps_m <- logistic.all$mid[logistic.all$year != myYear &
logistic.all$year >= startYearPSS]
ps_s <- logistic.all$sd[logistic.all$year != myYear &
logistic.all$year >= startYearPSS]
ps_alpha.log <- logistic.all$alpha[logistic.all$year != myYear &
logistic.all$year >= startYearPSS]
n_ps_m <- length(ps_m)
n_ps_s <- length(ps_s)
n_ps_alpha_log <- length(ps_alpha.log)
# Inits with small range; alpha is 1e5-1.1e5
inits<-  function(){
list(
"ps_alpha_curr" = runif(1,1e5,11e4),
"ps_mu_curr" = runif(1,172,175),
"ps_sd_curr" = runif(1,5,7),
"ps_alpha_hist" = runif(n_yearPSS,1e5,11e4),
"ps_mu_hist" = runif(n_yearPSS,172,175),
"ps_sd_hist" = runif(n_yearPSS,5,7),
"sigma" = runif(1,2,3),
"sigma_hist" = runif(n_yearPSS,2,3),
"beta" = runif(1,0,0.5),
"sigma_reg" = runif(1,0.2,0.5),
"alpha" = runif(1,10,11),
"beta_sst" = runif(1,-2,-1),
"sigma_t" = runif(1,2.5,3),
"alpha_t" = runif(1,177,178),
"beta_sst" = runif(1,-2,0),
"alpha_t" = runif(1,170,180)
)
}
inits_ll <- list(inits(), inits(), inits(), inits())
fit <- stan(file = file.path(dir.stan,paste("Yukon Inseason Forecast ",
model.version ,".stan", sep = "")),
data = list("PSS_mat"=PSS_mat,
"PSS_mat_all" = PSS_mat_all,
"cum_PSS_mat_all" = cum_PSS_mat_all,
"n_totalEOS"=n_totalEOS,
"totalEOS"=totalEOS,
"n_dayPSS"=n_dayPSS,
"dayPSS"=dayPSS,
"n_yearPSS"=n_yearPSS,
"yearPSS"=yearPSS,
"n_curr_PSS"=n_curr_PSS,
"curr_PSS"=curr_PSS,
# "n_hist_counts"=n_hist_counts,
"Pf" = pf,
"Pf_sigma" = pf_sigma,
"meanpropCAN" = meanGSI_vect,
"sd_meanpropCAN" = sdGSI_vect,
# "meanpropCAN" = GSI_avg_vect,
# "sd_meanpropCAN" = GSI_sd_vect,
# "mean_adj_PSS_mat" = PSS_mat_adj,
# "mean_adj_curr_PSS"= adj_curr_PSS,
# "PSS_cum_hist_mat" = PSS_cum_hist_mat,
# "cum_curr_PSS" = cum_curr_PSS,
# "meanpropCAN"= GSI_avg_vect,
# "sd_meanpropCAN" = GSI_sd_vect,
# "GSI_mean"= GSI_avg_vect,
# "GSI_sd" = GSI_sd_vect,
"Eagle_mat" = Eagle_mat,
"n_totalEOS" = n_totalEOS,
"totalEOS" = totalEOS,
"n_dayEagle" = n_dayEagle,
"dayEagle" = dayEagle,
"n_yearEagle" = n_yearEagle,
"yearEagle" = yearEagle,
"n_curr_Eagle" = n_curr_Eagle,
"curr_Eagle" = curr_Eagle,
"loc_eagle_years" = loc.eagle.years,
# "loc_pf_years" = loc.PF.years,
"n_yearsPF" = n_yearsPF,
"loc_pf_years_Eagle" = loc.PF.years.eagle,
"loc_pf_years_PSS" = loc.PF.years.PSS,
"paramA" = paramA,
"paramB" = paramB,
"curr_PSS_year_sd" = curr_PSS_year_sd,
"PSS_year_sd" = PSS_year_sd,
"propEagle" = prop_Eagle,
"mean_propEagle" = mean_prop_Eagle,
"may_sst_hist" = may.sst.hist,
"n_yearSST" = n_yearSST,
"may_sst_curr" = may.sst.curr,
"dev_hist" = deviations.mp,
"hist_MP" = hist.midpoint,
"myDay" = myDay,
"avg_mid" = log_mid,
"shape" = log_s,
"n_dayPSS_all" = n_dayPSS_all,
"PSS_mat_prop_all" = PSS_mat_prop_all,
"dayPSS_all" = dayPSS_all,
"mean_propPSS" = mean_propPSS,
"ps_mu"= ps_mu,
"ps_sd"= ps_sd,
"ps_alpha_norm"= ps_alpha_norm,
"n_ps_mu" = n_ps_mu,
"n_ps_sd" = n_ps_sd,
"n_ps_alpha" = n_ps_alpha,
"loc_allDays_myDay" = loc.allDays.myDay,
"n_sst" = n_sst,
"loc_devMP_allYears" = loc.devMP.allYears,
# "PSS_mat_all" = PSS_mat_all,
"ps_alpha_log"= ps_alpha.log,
"ps_m"= ps_m,
"ps_s"= ps_s,
"n_ps_m" = n_ps_m,
"n_ps_s" = n_ps_s,
"n_ps_alpha_log" = n_ps_alpha_log,
"cum_PSS_mat" = cum_PSS_mat,
"cum_curr_PSS" = cum_curr_PSS
),
init = inits_ll,
chains = n.chains,
# chains = 1,
iter = n.iter,
thin = n.thin,
# cores = n.chains,
cores = mc.cores,
control = list(max_treedepth = 25, adapt_delta = 0.99),
verbose = F,
save_warmup = T
)
gc()
#
#=================================================================================
# NOTES:
#
#
#
# Next steps:
#
#
#=================================================================================
library(rstan)
library(bayesplot)
library(tidyverse)
library(ggthemes)
library(viridis)
library(shinystan)
library(lubridate)
library(ggpubr)
library(gridExtra)
library(tidybayes)
library(wesanderson)
library(grid)
library(bbmle)
# Parralize for optimum model run time
rstan_options(auto_write = TRUE)
#
mc.cores = parallel::detectCores()
# Define Workflow Paths ============================================
wd <- "C:/Users/aaron/Desktop/Yukon Kings/Inseason Forcast Model"
setwd(wd)
dir.output <- file.path(wd,"output")
dir.figs <- file.path(wd,"figs")
dir.stan <- file.path(wd,"Stan")
dir.data <- file.path(wd,"Data")
dir.R <- file.path(wd,"R")
# Functions to run model, create basic plots, and get day of year
source(file = file.path(dir.R,"model_run_function LOO3.r"))
# Function to get retrospective stats
source(file = file.path(dir.R,"Retro Function.R"))
# EOS Can-orig reconstructed counts (NEW METHOD)
CAN_hist_new <- readRDS(file.path(dir.data,"Canadian Passage RR 21Mar23.RDS"))
# PSS Passage
PSS_hist <- readRDS(file = file.path(dir.data,"PSS passage Final 2022.RDS"))
# Eagle passage
Eagle_hist <- readRDS(file = file.path(dir.data, "Eagle passage 23Nov22.RDS"))
# Read in historical avg of GSI by strata (Naive estimator)
GSI_mean <- readRDS(file = file.path(dir.data,"Mean GSI by strata 2005-2020.RDS"))
# Read in historic preseason forecasts (2013 - current)
pf_hist <- readRDS(file.path(dir.data,"pf_ver3.1_14Aprl23_eagle+harvest.RDS"))
# Read in genetic stock identification (2005-2019)
# (adjusted to capture early and late runs)
GSI_by_year <- readRDS(file = file.path(dir.data,"GSI by year unadj 21Mar23.RDS"))
# Read in PSS observation error estimates
PSS_sd <- readRDS(file = file.path(dir.data,"PSS SD 1995_2021.RDS"))
# Shape parameters for fitting curves
logistic.all <- read.csv(file = file.path(dir.output,"logistic curve parameters All Chinook 1995_2022.csv"))
normal.all <- read.csv(file = file.path(dir.output,"normal curve parameters All Chinook 1995_2022.csv"))
# SST May data
norton.sst <- readRDS(file = file.path(dir.data,"norton.sst 6Apr23.RDS"))
# Control Section ######################
model.version <- "6.prop"
# MCMC Parameters
n.chains <- 4
n.iter <- 2000 #30000;#5e4
n.thin <- 2
# Test days used in full season run (every 5 days starting June 2)
testDays <- seq(from = 153, to = 243, by = 5)
# Years included in full run (Used for retro with Eagle+Harvest Can-orig counts)
testYears <- c(2007:2022)
# # List to store outputs
outputList<-list()
# Increse memory limit for runs of model versions 4 and higher
# memory.limit(size = 60000)
options(warn = 1)
for(y in c(testYears)){
for(d in c(testDays)){
outputList[[paste("",y,"_",d, sep = "")]]<-InSeasonProjection(model.version = model.version,
myYear = y,
myDay = d,
n.chains = n.chains,
CAN_hist = CAN_hist_new,
pf_hist = pf_hist,
PSS_hist = PSS_hist,
PSS_sd = PSS_sd,
norton.sst = norton.sst,
n.thin = n.thin,
n.iter = n.iter,
GSI_by_year = GSI_by_year,
Eagle_hist = Eagle_hist,
normal = FALSE,
logistic = FALSE,
prior.df.log = logistic.all,
prior.df.norm = normal.all,
multiplier = 1,
startDayPSS = 148,
startYearPSS = 2005)
print(paste("Day =",d,"Year =",y))
} #dloop
print(paste("Finally done with year",y))
} #yloop
# Save output
saveRDS(object = outputList, file = file.path(dir.output, "Ver6prop_rr_trunc_17July23.RDS"))
outputlist_ver1RR_T <- readRDS(file = file.path(dir.output, "Ver1_RRCAN_trunc_long_3May23.RDS"))
outputlist_ver11RR_T <- readRDS(file = file.path(dir.output, "Ver11_rr_trunc_long_5May23.RDS"))
